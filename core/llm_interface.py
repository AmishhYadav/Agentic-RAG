import json
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional

import boto3


class LLMProvider(ABC):
    """Abstract base class for LLM providers."""

    @abstractmethod
    def generate(
        self, system_prompt: str, user_prompt: str, temperature: float = 0.0
    ) -> str:
        """Generates a response from the LLM."""
        pass


class MockLLM(LLMProvider):
    """Local rule-based LLM for development and testing."""

    def generate(
        self, system_prompt: str, user_prompt: str, temperature: float = 0.0
    ) -> str:
        """
        Simulates LLM responses based on keywords in the prompt.
        This allows testing agent logic without actual model calls.
        """
        user_prompt_lower = user_prompt.lower()

        # Simulation for Query Agent
        if "analyze the following user query" in system_prompt.lower():
            # Trigger retrieval for domain-specific keywords
            keywords = [
                "retrieval",
                "complex",
                "bedrock",
                "aws",
                "security",
                "rag",
                "iam",
                "cloud",
            ]
            if any(k in user_prompt_lower for k in keywords):
                return json.dumps(
                    {
                        "needs_retrieval": True,
                        "retrieval_strategy": "vector_similarity",
                        "reasoning": f"Query contains domain keywords ({[k for k in keywords if k in user_prompt_lower]}) requiring context.",
                    }
                )
            else:
                return json.dumps(
                    {
                        "needs_retrieval": False,
                        "retrieval_strategy": None,
                        "reasoning": "Query appears to be general conversation or out of knowledge scope.",
                    }
                )

        # Simulation for Synthesis Agent
        if "synthesize an answer" in system_prompt.lower():
            if "bedrock" in user_prompt_lower:
                return "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI."
            elif "security" in user_prompt_lower or "iam" in user_prompt_lower:
                return "AWS Identity and Access Management (IAM) allows you to securely control access to AWS services and resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources."
            elif any(
                greeting in user_prompt_lower.split()
                for greeting in ["hi", "hello", "hey"]
            ):
                return "Hello! I am ready to help you with questions about AWS, Bedrock, and Security. What would you like to know?"

            # Check if context was actually provided in the system prompt
            elif "Retrieved Context:" in system_prompt:
                # Extract a bit of context or just return a success message
                return "Based on the retrieved context, I can confirm that the system has found relevant documents to answer your query regarding AWS/Bedrock/Security. (Simulated RAG Answer)"

            else:
                return "Based on the retrieved context, this is a simulated answer generated by the MockLLM. (Context was irrelevant or missing for specific mock response)."

        # Simulation for Verifier Agent
        if "verify the following answer" in system_prompt.lower():
            return json.dumps(
                {
                    "is_valid": True,
                    "reasoning": "The answer is supported by the mock evidence provided.",
                }
            )

        return "Mock LLM Response: I received your input but don't have a specific rule for it."


class BedrockLLM(LLMProvider):
    """Production LLM using Amazon Bedrock."""

    def __init__(self, region_name: str, model_id: str):
        self.client = boto3.client(
            service_name="bedrock-runtime", region_name=region_name
        )
        self.model_id = model_id

    def generate(
        self, system_prompt: str, user_prompt: str, temperature: float = 0.0
    ) -> str:
        """Invokes Claude 3 via Bedrock API."""

        # Construct the body for Claude 3 (Anthropic Messages API)
        body = json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "temperature": temperature,
                "system": system_prompt,
                "messages": [{"role": "user", "content": user_prompt}],
            }
        )

        try:
            response = self.client.invoke_model(modelId=self.model_id, body=body)
            response_body = json.loads(response.get("body").read())
            return response_body.get("content")[0].get("text")

        except Exception as e:
            # logic to handle throttling or errors
            print(f"Error invoking Bedrock: {e}")
            return f"Error: {str(e)}"


# Factory to get LLM instance
def get_llm(config: Dict[str, Any]) -> LLMProvider:
    provider = config.get("provider", "mock")
    if provider == "bedrock":
        return BedrockLLM(
            region_name=config.get("region", "us-east-1"),
            model_id=config.get(
                "model_id", "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
            ),
        )
    else:
        return MockLLM()
